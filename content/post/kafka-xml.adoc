---
title: 'Ingesting XML data into Kafka'
date: "2020-09-25T12:09:41+01:00"
image: "/images/2020/09/IMG_6833.jpeg"
thumbnail: "/images/2020/09/IMG_6861.jpeg"
draft: true
credit: "@rmoff"
categories:
- XML
- kafkacat
- xq
- Kafka Connect
---

:source-highlighter: rouge
:icons: font
:rouge-css: style
:rouge-style: github

XML has been around for 20+ years, and whilst other ways of serialising our data have gained popularity in more recent times (such as JSON, Avro, and Protobuf), XML is not going away soon. Part of that is down to technical reasons (clearly defined and documented schemas), and part of it is simply down to enterprise inertia - having adopted XML for systems in the last couple of decades, they're not going to be changing now just for some short-term fad. See also COBOL. 

Given this, it's not an uncommon question to see asked in the Kafka community how one can get data from a source system (usually through https://rmoff.dev/what-is-kafka-connect[Kafka Connect]) that's in XML form into a Kafka topic. 

== ü§î What are we expecting to see in the Kafka topic? 

Let's start from the basics. Kafka messages are just bytes, so we can put whatever we want into it. We can dump XML into a Kafka topic, and now the Kafka topic has XML in it. But what are we expecting to do with that data? Unless our consuming application literally wants a stream of XML (in which case you are done now) then we need find a way to convert the XML data and its schema into a form that a Kafka consumer can read and access the actual schema. 

An XML message stored as plain text in Kafka: 

[width="100%",cols="2",options="header"]
|===
|Source | Kafka message
|`<?xml version='1.0' encoding='UTF-8'?>
<dataset> <record> <name>Edinburgh NCP</name> <space>E63</space> <occupied>false</occupied> </record> <record> <name>Bournemouth NCP</name> <space>E88</space> <occupied>true</occupied> </record> </dataset>` | `<?xml version='1.0' encoding='UTF-8'?> <dataset> <record> <name>Edinburgh NCP</name> <space>E63</space> <occupied>false</occupied> </record> <record> <name>Bournemouth NCP</name> <space>E88</space> <occupied>true</occupied> </record> </dataset>`
|===

It's not much more different from a payload that looks like this

[width="100%",cols="2",options="header"]
|===
|Source | Kafka message
|`Bacon ipsum dolor amet strip steak fatback porchetta` | `Bacon ipsum dolor amet strip steak fatback porchetta`
|===

*It's just a string*, and when it comes to a consuming application reading the message from the Kafka topic the application will need to know how to interpret that data, whether parsing the XML with an XSD, or figuring out some piggy-goodness üê∑.

What we actually want to do is store the message in Kafka as a payload plus schema. That then gives us a message that logically looks like this: 

[width="100%",cols="1,2a",options="header"]
|===
|Source | Kafka message
|`<?xml version='1.0' encoding='UTF-8'?>
<dataset> <record> <name>Edinburgh NCP</name> <space>E63</space> <occupied>false</occupied> </record> <record> <name>Bournemouth NCP</name> <space>E88</space> <occupied>true</occupied> </record> </dataset>` | 

[width="100%",cols="3",options="header"]
!===
!name!space!occupied
!Edinburgh NCP!E63!false
!Bournemouth NCP!E88!true
!===

|===

_If you look closely we're making some assumptions about the payload handling. We've taken one XML message and assumed that the `<dataset> <record>` is a wrapper, holding two records. It could be we want to hold the whole thing as a single message - and this is where we get into the nitty gritty of reserialising formats, because there's a bunch of assumptions and manual steps that need to be verified_. 

=== Schemas, Schmeeeemas

Who cares about schemas? Me. You. Anyone wanting to build pipelines and applications around Kafka that are decoupled from the source, and not beholden to it to find out about the data coming from it. Given the example in the section above, we could take the final rendering with the `name`/`space`/`occupied` fields, hook that up to the JDBC sink, and stream that directly into a database - and create the target table too, because we have the schema necessary to execute the DDL. 

XML is self-documenting with an XSD, but it's not a generally-supported serde in the Kafka ecosystem. For that, you want to look at Avro, Protobuf, or JSON Schema. The Confluent Schema Registry supports all three, and provides serdes for any producer & consumer application. It plugs in directly to Kafka Connect and ksqlDB too, and it enables you to build "plug and play" data pipelines that *_just work_*. 

*Why not just JSON?* I mean, with JSON we can have messages that look like this: 

[source,javascript]
----
{
    "name": "Edinburgh NCP",
    "space": "E63",
    "occupied": "false"
}
----

It _looks_ like there's a schema, doesn't it? We can store this JSON data on a Kafka message, and isn't that going to be good enough? Well, not really - because we can only _infer_ (which is a posh way of saying 'guess') the schema. We can _assume_ that there are three columns, and that they can't be null, and they _look_ like they're `VARCHAR`, although `occupied` could be a boolean - but we don't *know*. 

If we want to use the data we have to specify the actual schema at the point at which we want to consume it (which in practice is going to mean coupling ourselves back to the team/org that wrote the data): 

[source,sql]
----
CREATE STREAM carpark_json (name VARCHAR, 
                            space VARCHAR, 
                            occupied VARCHAR) 
                      WITH (KAFKA_TOPIC='carpark_json', 
                      VALUE_FORMAT='JSON');
----

Contrast this to serialising the data on a Kafka topic with a format that enables us to register an actual schema. Now when it comes to use the data we *know* all of these things (fields, data types, defualts, nullability, etc) - and it's available to any consumer too. Check out this example, in which the consumer is ksqlDB: 

[source,sql]
----
-- Note that we don't have to type in the schema
-- This is because the consuming application (ksqlDB here)
-- can retrieve the full schema from the Schema Registry
CREATE STREAM carpark_proto 
    WITH (KAFKA_TOPIC='carpark_proto', 
          VALUE_FORMAT='PROTOBUF');

-- Here's the schema:
ksql> DESCRIBE carpark_proto;

Name                 : CARPARK_PROTO
 Field    | Type
----------------------------
 NAME     | VARCHAR(STRING)
 SPACE    | VARCHAR(STRING)
 OCCUPIED | VARCHAR(STRING)
----------------------------
----

Learn more about the importance of schemas here: 

* üé• https://youtu.be/WpfJ86_DYfY?t=2209[Serialisation and Schemas (Kafka as a Platform: the Ecosystem from the Ground Up - NDC Oslo 2020)]
* üé• https://rmoff.dev/qcon-schemas[Streaming Microservices: Contracts & Compatibility - Gwen Shapira - QCon]
* ‚úçÔ∏è https://www.confluent.io/blog/schemas-contracts-compatibility/[Schemas, Contracts, and Compatibility]
* ‚úçÔ∏è https://www.confluent.io/blog/schema-registry-kafka-stream-processing-yes-virginia-you-really-need-one/[Yes, Virginia, You Really Do Need a Schema Registry]

For the rest of this article we're going to assume that you want to get the payload from the XML into Kafka into a form in which the schema is also declared and available to use for consuming applications. 

== XML into Kafka - Option 1: The Dirty Hack

What would a blog post on `rmoff.net` be if it didn't include the dirty hack option? üòÅ

_The secret to dirty hacks is that they are often rather effective and when needs must, they can suffice. If you're prototyping and need to *JFDI*, a dirty hack is just fine. If you're looking for code to run in Production, then a dirty hack probably is not fine._

Let's assume we've got XML data sat on a REST endpoint somewhere that we can poll, https://tfl.gov.uk/tfl/syndication/feeds/cycle-hire/livecyclehireupdates.xml[like this] (courtesy of [TfL OpenData]). We're going to use the power of https://en.wikipedia.org/wiki/Pipeline_(Unix)[unix pipelines] to string together some powerful tools: 

* `curl` to pull the data from the REST endpoint
* https://github.com/jeffbr13/xq[`xq`] - like the well-known https://stedolan.github.io/jq/[`jq`] tool, but for XML, and outputs JSON
* https://github.com/edenhill/kafkacat[`kafkacat`] - takes input from `stdin` and produces it to a Kafka topic

=== Wrangling the XML data and streaming it into Kafka 

Let's start by checking what we actually want to send to Kafka. The raw payload looks like this: 

[source,xml]
----
<?xml version="1.0" encoding="utf-8"?>
<stations lastUpdate="1601312340962" version="2.0">
    <station>
        <id>1</id>
        <name>River Street , Clerkenwell</name>
        <terminalName>001023</terminalName>
        <lat>51.52916347</lat>
‚Ä¶
    </station>
    <station>
        <id>2</id>
        <name>Phillimore Gardens, Kensington</name>
        <terminalName>001018</terminalName>
        <lat>51.49960695</lat>
‚Ä¶
‚Ä¶
----


Using `xq` we use the same kind of construction as we would with `jq` to construct a target JSON object: 

[source,bash]
----
curl --show-error --silent https://tfl.gov.uk/tfl/syndication/feeds/cycle-hire/livecyclehireupdates.xml | \
    xq '.' 
----

This gives us a JSON structure that looks like this

[source,javascript]
----
{
  "stations": {
    "@lastUpdate": "1601462461108",
    "@version": "2.0",
    "station": [
      {
        "id": "1",
        "name": "River Street , Clerkenwell",
        "terminalName": "001023",
        "lat": "51.52916347",
        ‚Ä¶   
      },
      {
        "id": "2",
        "name": "Phillimore Gardens, Kensington",
        "terminalName": "001018",
        "lat": "51.49960695",
      ‚Ä¶
      },
----

We need to decide how to carve up the data, since we've effectively got a batch of data here and Kafka works on the concept of messages/records. Therefore we're going to do this: 

* Take each `station` element as its own message
* Add in the `lastUpdate` value from the `stations` element into each `station` message (i.e. denormalise the payload somewhat)

We can use some `xq` magic to do this, extracting each element from the station array into its own root-level object (`.stations.station[]`) and adding in the `lastUpdate` field (`+ {lastUpdate: .stations."@lastUpdate"}`). If you want to learn more about the power of `jq` (on which `xq` is modelled) you can https://jqplay.org/s/kzU67eW4k0[try out this code here].

So with the source REST API data piped through `xq` we get this: 

[source,bash]
----
curl --show-error --silent https://tfl.gov.uk/tfl/syndication/feeds/cycle-hire/livecyclehireupdates.xml | \
    xq '.stations.station[] + {lastUpdate: .stations."@lastUpdate"}'
----

[source,javascript]
----
{
  "id": "1",
  "name": "River Street , Clerkenwell",
  "terminalName": "001023",
  "lat": "51.52916347",
‚Ä¶
  "lastUpdate": "1601462700830"
}
{
  "id": "2",
  "name": "Phillimore Gardens, Kensington",
  "terminalName": "001018",
  "lat": "51.49960695",
‚Ä¶
  "lastUpdate": "1601462700830"
}
----

If we send the data to Kafka in this form using kafkacat we'll end up with garbled data because each line will be taken as its own message (the line break would act as the default message delineator). To fix this we'll use the `-c` flag with `xq` which `c`ompacts the output: 

[source,bash]
----
curl --show-error --silent https://tfl.gov.uk/tfl/syndication/feeds/cycle-hire/livecyclehireupdates.xml | \
    xq -c '.stations.station[] + {lastUpdate: .stations."@lastUpdate"}'
----

[source,javascript]
----
{"id":"1","name":"River Street , Clerkenwell","terminalName":"001023","lat":"51.52916347",‚Ä¶,"lastUpdate":"1601462880994"}
{"id":"2","name":"Phillimore Gardens, Kensington","terminalName":"001018","lat":"51.49960695",‚Ä¶,"lastUpdate":"1601462880994"}
----

We're now in a position to stream this into a Kafka topic, by adding `kafkacat` to the pipeline: 

[source,bash]
----
curl --show-error --silent https://tfl.gov.uk/tfl/syndication/feeds/cycle-hire/livecyclehireupdates.xml | \
    xq -c '.stations.station[] + {lastUpdate: .stations."@lastUpdate"}' | \
    kafkacat -b localhost:9092 -t livecyclehireupdates_01 -P
----

We can use `kafkacat` as a consumer too, here specifying `c1` to consume just one message:

[source,bash]
----
kafkacat -b localhost:9092 -t livecyclehireupdates_01 -C -c1
----

[source,javascript]
----
{"id":"1","name":"River Street , Clerkenwell","terminalName":"001023","lat":"51.52916347",‚Ä¶,"lastUpdate":"1601464200733"}
----

=== What about keys? 

Kafka messages are key/value, and we've specified a value but no key. This is where the hack gets just that little bit more hacky. We're going to use xq to write the `id` field from the XML payload as a prefix to each message, with a separator so that kafkacat can identify where the key ends and the value stops. 

Our `xq` invocation now looks like this: 

[source,bash]
----
xq -rc --arg sep $'\x1c' '.stations.station[] + { lastUpdate: .stations."@lastUpdate"} |  [ .id + $sep, tostring] |  join("")'
----

The `--arg sep $'\x1c'` declares a variable called `sep` which holds a http://www.fileformat.info/info/unicode/char/001c/index.htm[control  character] that we will use as the separator. The output looks like this: 

[source,bash]
----
00000000  31 1c 7b 22 69 64 22 3a  22 31 22 2c 22 6e 61 6d  |1.{"id":"1","nam|
00000010  65 22 3a 22 52 69 76 65  72 20 53 74 72 65 65 74  |e":"River Street|
00000020  20 2c 20 43 6c 65 72 6b  65 6e 77 65 6c 6c 22 2c  | , Clerkenwell",|
00000030  22 74 65 72 6d 69 6e 61  6c 4e 61 6d 65 22 3a 22  |"terminalName":"|
00000040  30 30 31 30 32 33 22 2c  22 6c 61 74 22 3a 22 35  |001023","lat":"5|
----

Check out the first bytes - `31` represents `1` which is the `id`, and then `1c` is the separator. 

We can use this with kafkacat like this: 

[source,bash]
----
curl --show-error --silent https://tfl.gov.uk/tfl/syndication/feeds/cycle-hire/livecyclehireupdates.xml | \
    xq -rc --arg sep $'\x1c' '.stations.station[] + { lastUpdate: .stations."@lastUpdate"} |  [ .id + $sep, tostring] |  join("")' | \
    kafkacat -b localhost:9092 -t livecyclehireupdates_06 -P -K$'\x1c'
----

Checking the data in the topic with kafkacat we can see that we've now set the key as we wanted, taking the value of the `id` field: 

[source,bash]
----
kafkacat -b localhost:9092 \
         -t livecyclehireupdates_06 \
         -C -c2 \
         -f 'Key: %k, payload: %s\n'
----

[source,javascript]
----
Key: 1, payload: {"id":"1","name":"River Street , Clerkenwell","terminalName":"001023","lat":"51.52916347",‚Ä¶"lastUpdate":"1601485080861"}
Key: 2, payload: {"id":"2","name":"Phillimore Gardens, Kensington","terminalName":"001018","lat":"51.49960695",‚Ä¶"lastUpdate":"1601485080861"}
----


=== We've got data, but no schema

So we now have a Kafka topic with the XML-sourced data in it, but held in plain JSON. For it to be really useful, we want it in a form that is usable by consumers with little-or-no input from the producer of the data, and for that we want to declare and store the schema. I'm going to use https://ksqldb.io[ksqlDB] for this - you can use other stream processing options such as Kafka Streams if you'd rather. 

