---
draft: false
title: 'ðŸŽ„ Twelve Days of SMT ðŸŽ„ - Day 4: RegExRouter'
date: "2020-12-11T16:40:18Z"
image: "/images/2020/12/smt_day4.jpg"
thumbnail: "/images/2020/12/smt_day4_thumb.jpg"
credit: "https://twitter.com/rmoff/"
categories:
- Kafka Connect
- Single Message Transform
- TwelveDaysOfSMT
---

:source-highlighter: rouge
:icons: font
:rouge-css: style
:rouge-style: github

If you want to change the topic name to which a source connector writes, or object name that's created on a target by a sink connector, the https://docs.confluent.io/platform/current/connect/transforms/regexrouter.html[`RegExRouter`] is exactly what you need. 

To use the Single Message Transform you specify the pattern in the topic name to match, and its replacement. To drop a prefix of `test-` from a topic you would use: 

[source,javascript]
----
"transforms"                             : "dropTopicPrefix",
"transforms.dropTopicPrefix.type"        : "org.apache.kafka.connect.transforms.RegexRouter",
"transforms.dropTopicPrefix.regex"       : "test-(.*)",
"transforms.dropTopicPrefix.replacement" : "$1"
----

<!--more-->

{{< youtube btphhOn5hcw >}}

https://github.com/confluentinc/demo-scene/blob/master/kafka-connect-single-message-transforms[ðŸ‘¾ Demo code]

== Changing the topic name to which a source connector writes

image::https://imgs.xkcd.com/comics/regular_expressions.png[XKCD]

Source connectors will stream data to a Kafka topic based on properties define in the particular connector. For example, the JDBC source connector uses the table name and prefixes it with the mandatory value configured in `topic.prefix`. Other connectors will use the name of the source message queue being read from, the source file, etc etc. 

Often, you'll want to route data to a topic name that matches https://riccomini.name/how-paint-bike-shed-kafka-topic-naming-conventions[conventions that you have in your organisation for topic naming]. Here's an example of a JDBC source connector, and we want to drop the prefix that it uses: 

[source,javascript]
----
{
  "connector.class"                       : "io.confluent.connect.jdbc.JdbcSourceConnector",
  "connection.url"                        : "jdbc:mysql://mysql:3306/demo",
  "connection.user"                       : "mysqluser",
  "connection.password"                   : "mysqlpw",
  "topic.prefix"                          : "mysql-02-",
  "poll.interval.ms"                      : 1000,
  "tasks.max"                             : 1,
  "table.whitelist"                       : "customers",
  "mode"                                  : "incrementing",
  "incrementing.column.name"              : "id",
  "validate.non.null"                     : false,
  "transforms"                            : "dropTopicPrefix",
  "transforms.dropTopicPrefix.type"       : "org.apache.kafka.connect.transforms.RegexRouter",
  "transforms.dropTopicPrefix.regex"      : "mysql-02-(.*)",
  "transforms.dropTopicPrefix.replacement": "$1"
}
----

This is using RegEx to match the prefix `mysql-02-` and to store everything else `.*` in a capture group `(` `)`, which is then referenced in the replacement `$1`. 

To learn more about RegEx, and experiment with patterns, check out the excellent https://regexr.com/5i7cc[RegExr.com]

== Changing the object name to which a source connector writes

Many sink connectors will use the topic name as the basis for the naming of the target object that it populates. The JDBC Sink connector creates a table named after the topic. The Elasticsearch sink connector creates an index named after the topic. And so on. 

You can use the `RegExRouter` to customise the name of the object that sink connectors that follow this pattern will write to. 

Here's an example of streaming data to MySQL, using the https://www.confluent.io/hub/confluentinc/kafka-connect-jdbc[JDBC sink connector]. _(See also ðŸŽ¥ https://rmoff.dev/kafka-jdbc-video[Kafka Connect in Action : JDBC Sink] (ðŸ‘¾ link:../kafka-to-database/README.adoc[`demo code`]) and ðŸŽ¥ https://rmoff.dev/ksqldb-jdbc-sink-video[ksqlDB & Kafka Connect JDBC Sink in action] (ðŸ‘¾ link:../kafka-to-database/ksqldb-jdbc-sink.adoc[`demo code`])_

We're going to read data from a topic called `day4-transactions`: 

[source,javascript]
----
curl -i -X PUT -H "Accept:application/json" \
    -H  "Content-Type:application/json" http://localhost:8083/connectors/sink-jdbc-mysql-day4-transactions-00/config \
    -d '{
          "connector.class"               : "io.confluent.connect.jdbc.JdbcSinkConnector",
          "connection.url"                : "jdbc:mysql://mysql:3306/demo",
          "connection.user"               : "mysqluser",
          "connection.password"           : "mysqlpw",
          "topics"                        : "day4-transactions",
          "tasks.max"                     : "4",
          "auto.create"                   : "true",
          "auto.evolve"                   : "true"
        }'
----

This works; you get a table created in MySQL:

[source,sql]
----
mysql> show tables;
+-------------------+
| Tables_in_demo    |
+-------------------+
| day4-transactions |
+-------------------+
1 row in set (0.00 sec)
----

What data's in the table? 

[source,sql]
----
mysql> select * from day4-transactions;
ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '-transactions' at line 1
----

Turns out a hyphen in the table name does not make your life easy in MySQL. You can quote is with a backtick, but it is not ideal

[source,sql]
----
mysql> select * from `day4-transactions` LIMIT 1;
+-----------+-------+---------------------------+
| card_type | cost  | item                      |
+-----------+-------+---------------------------+
| switch    | 98.77 | Westmalle Trappist Tripel |
+-----------+-------+---------------------------+
1 row in set (0.00 sec)
----

By default the JDBC Sink connector takes the topic name as the name of the table to create. Let's modify the above connector to route data to a table called `transactions` instead, and drop the `day4-` prefix. 

[source,javascript]
----
curl -i -X PUT -H "Accept:application/json" \
    -H  "Content-Type:application/json" http://localhost:8083/connectors/sink-jdbc-mysql-day4-transactions-00/config \
    -d '{
          "connector.class"                        : "io.confluent.connect.jdbc.JdbcSinkConnector",
          "connection.url"                         : "jdbc:mysql://mysql:3306/demo",
          "connection.user"                        : "mysqluser",
          "connection.password"                    : "mysqlpw",
          "topics"                                 : "day4-transactions",
          "tasks.max"                              : "4",
          "auto.create"                            : "true",
          "auto.evolve"                            : "true",
          "transforms"                             : "dropTopicPrefix",
          "transforms.dropTopicPrefix.type"        : "org.apache.kafka.connect.transforms.RegexRouter",
          "transforms.dropTopicPrefix.regex"       : "day4-(.*)",
          "transforms.dropTopicPrefix.replacement" : "$1"
        }'
----

Since we've `PUT` the above configuration it updates the existing connector, and now we have a table in MySQL without the `day4-` prefix that's much easier to work with: 

[source,sql]
----
mysql> show tables;
+-------------------+
| Tables_in_demo    |
+-------------------+
| day4-transactions |
| transactions      |
+-------------------+
2 rows in set (0.00 sec)

mysql> select * from transactions limit 1;
+-----------+-------+-----------------+
| card_type | cost  | item            |
+-----------+-------+-----------------+
| dankort   | 27.12 | Sapporo Premium |
+-----------+-------+-----------------+
1 row in set (0.00 sec)
----



== Example - JDBC Sink connector 

_See also ðŸŽ¥ https://rmoff.dev/kafka-jdbc-video[Kafka Connect in Action : JDBC Sink] (ðŸ‘¾ link:../kafka-to-database/README.adoc[`demo code`]) and ðŸŽ¥ https://rmoff.dev/ksqldb-jdbc-sink-video[ksqlDB & Kafka Connect JDBC Sink in action] (ðŸ‘¾ link:../kafka-to-database/ksqldb-jdbc-sink.adoc[`demo code`])_

Given a source message that looks like this: 

[source,javascript]
----
{
  "FULL_NAME": "Opossum, american virginia",
  "ADDRESS": {
    "STREET":  "20 Acker Terrace"
    "CITY":  "Lynchburg"
    "COUNTY_OR_STATE": "Virginia"
    "ZIP_OR_POSTCODE": "24515"
  }
}
----

We can't load it directly into a database because databases expect flat structures. If we try to load it as it is the JDBC Sink connector will fail and throw an error: 

[source,bash]
----
â€¦(STRUCT) type doesn't have a mapping to the SQL database column type
----

So we use the Single Message Transform to flatten the source payload:

[source,javascript]
----
curl -i -X PUT -H "Accept:application/json" \
    -H  "Content-Type:application/json" http://localhost:8083/connectors/sink-jdbc-mysql-day3-customers-00/config \
    -d '{
          "connector.class"               : "io.confluent.connect.jdbc.JdbcSinkConnector",
          "connection.url"                : "jdbc:mysql://mysql:3306/demo",
          "connection.user"               : "mysqluser",
          "connection.password"           : "mysqlpw",
          "topics"                        : "day3-customers",
          "tasks.max"                     : "4",
          "auto.create"                   : "true",
          "auto.evolve"                   : "true",
          "transforms"                    : "flatten",
          "transforms.flatten.type"       : "org.apache.kafka.connect.transforms.Flatten$Value",
          "transforms.flatten.delimiter"  : "_"
        }'
----

This will work, and you can now see the data in MySQL: 

[source,sql]
----
mysql> describe `day3-customers`;
+-------------------------+------+------+-----+---------+-------+
| Field                   | Type | Null | Key | Default | Extra |
+-------------------------+------+------+-----+---------+-------+
| FULL_NAME               | text | YES  |     | NULL    |       |
| ADDRESS_STREET          | text | YES  |     | NULL    |       |
| ADDRESS_CITY            | text | YES  |     | NULL    |       |
| ADDRESS_COUNTY_OR_STATE | text | YES  |     | NULL    |       |
| ADDRESS_ZIP_OR_POSTCODE | text | YES  |     | NULL    |       |
+-------------------------+------+------+-----+---------+-------+
5 rows in set (0.00 sec)
----

[source,sql]
----
mysql> select * from `day3-customers`;
+----------------------------+-----------------------------+--------------+-------------------------+-------------------------+
| FULL_NAME                  | ADDRESS_STREET              | ADDRESS_CITY | ADDRESS_COUNTY_OR_STATE | ADDRESS_ZIP_OR_POSTCODE |
+----------------------------+-----------------------------+--------------+-------------------------+-------------------------+
| Opossum, american virginia | 20 Acker Terrace            | Lynchburg    | Virginia                | 24515                   |
| Red deer                   | 53 Basil Terrace            | Lexington    | Kentucky                | 40515                   |
| Laughing kookaburra        | 84 Monument Alley           | San Jose     | California              | 95113                   |
| American bighorn sheep     | 326 Sauthoff Crossing       | San Antonio  | Texas                   | 78296                   |
| Skua, long-tailed          | 7 Laurel Terrace            | Manassas     | Virginia                | 22111                   |
| Fox, bat-eared             | 2946 Daystar Drive          | Jamaica      | New York                | 11431                   |
| Greater rhea               | 97 Morning Way              | Charleston   | West Virginia           | 25331                   |
| Vervet monkey              | 7615 Brown Park             | Chicago      | Illinois                | 60681                   |
| White spoonbill            | 7 Fulton Parkway            | Asheville    | North Carolina          | 28805                   |
| Sun gazer                  | 61 Lakewood Gardens Parkway | Pensacola    | Florida                 | 32590                   |
+----------------------------+-----------------------------+--------------+-------------------------+-------------------------+
10 rows in set (0.00 sec)
----

